{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBERTa MLM and Tokenizer train for Text generation DatasetByText.ipynb","provenance":[{"file_id":"https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb","timestamp":1612196531906}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"M1oqh0F6W3ad"},"source":["# Train a language model (Masked Language Modelling) from scratch using Huggingface Transformers and a custom tokenizer\n","\n","### Inspired from the great notebook by Huggingfce (link to blogpost [link](https://huggingface.co/blog/how-to-train)).\n","\n","# Brief Introduction\n","This blog post is the first part of a series where we want to create a product names generator using a transformer model. For a few weeks I was investigating different models and alternatives in Huggingface to train a text generation model. We have a short list of products with their description and our goal is to obtain the name of the product. I did some experiments with the Transformer model in Tensorflow as well as the T5 summarizer. Finally, in order to deepen the use of Huggingface transformers, I decided to approach the problem with a somewhat more complex approach, an encoder decoder model. Maybe it was not the best option but I wanted to learn new things about huggingface Transformers. In the next post of the series we will introduce you deeper in this concept.\n","\n","Here, in this first part, we will show how to train a tokenizer from scratch and how to use Masked Language Modeling technique to create a RoBERTa model. This personalized model will become the base model for our future encoder Decoder model.\n","\n","# Our Solution\n","For our experiment we are going to train from scratch a RoBERTa model, it will become the encoder and the decoder of a future model. But our domain is very specific, words and concepts about clothes, shapes, colors, … Therefore, we are interested in defining our own tokenizer created from our specific vocabulary, avoiding to include more common words from other domains or use cases which are irrelevant for our final purpose.\n","\n","*We can describe our training phase in three main steps*:\n","- Create and train a byte-level, **Byte-pair encoding tokenizer** with the same special tokens as RoBERTa\n","- Train a RoBERTa model from scratch using **Masked Language Modeling**, MLM.\n","- Warm start and **fine tune an encoder decoder model** based on our RoBERTa pretrained model.\n","\n","In this post we’ll demo how to train a “small” RoBERTa model (6 layers, 768 hidden size, 12 attention heads) – that’s the same number of layers & heads as DistilBERT – on a language from a clothing shop. And in a next notebook, we’ll then fine-tune the model on a downstream task of text generation.\n"]},{"cell_type":"markdown","metadata":{"id":"flij-vszUsQm"},"source":["# Loading the libraries"]},{"cell_type":"code","metadata":{"id":"mc6KQfWIUuZB"},"source":["import os\n","import pandas as pd\n","import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tvVMVDYjUaRt"},"source":["# Loading the datasets\n","\n","As we mentioned before, our dataset contains around 31.000 items, about clothes from an important retailer, including a long product description and a short product name, our target variable. First, we execute a exploratory data analysis and we can observe that the count of rows with outliers values is a small number. The count of words looks like a left skewed distribution, 75% of rows in the range 50–60 words and a maximum about 125 words. The target variable contains about 3 to 6 words."]},{"cell_type":"code","metadata":{"id":"HOk4iZ9YZvec","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626950206838,"user_tz":-120,"elapsed":36404,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"1eb62c31-490a-467b-b0aa-68edba51da04"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5TwDqHbL9SY0"},"source":["Set the variables to the data folders:"]},{"cell_type":"code","metadata":{"id":"f2tjpFnyU-8T"},"source":["#Set the path to the data folder, datafile and output folder and files\n","root_folder = '/content/drive/My Drive/'\n","data_folder = os.path.abspath(os.path.join(root_folder, 'datasets/text_gen_product_names'))\n","model_folder = os.path.abspath(os.path.join(root_folder, 'Projects/text_generation_names/RoBERTaML'))\n","output_folder = os.path.abspath(os.path.join(root_folder, 'Projects/text_generation_names'))\n","tokenizer_folder = os.path.abspath(os.path.join(root_folder, 'Projects/text_generation_names/TokRoBERTa'))\n","\n","test_filename='cl_test_descriptions.csv'\n","datafile= 'product_names_desc_cl_train.csv'\n","outputfile = 'submission.csv'\n","\n","datafile_path = os.path.abspath(os.path.join(data_folder,datafile))\n","testfile_path = os.path.abspath(os.path.join(data_folder,test_filename))\n","outputfile_path = os.path.abspath(os.path.join(output_folder,outputfile))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nx_diDArVIHa"},"source":["Load the train datafile with the product descriptions and names:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MqxDq-LcVL0T","executionInfo":{"status":"ok","timestamp":1626950335969,"user_tz":-120,"elapsed":1492,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"88058734-99f5-4e8c-c264-d4893e973211"},"source":["# Load the train dataset\n","train_df=pd.read_csv(datafile_path, header=0, usecols=[0,1])\n","# Show the count of rows\n","print('Num Examples: ',len(train_df))\n","print('Null Values\\n', train_df.isna().sum())\n","# Drop rows with Null values \n","train_df.dropna(inplace=True)\n","print('Num Examples: ',len(train_df))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Num Examples:  31593\n","Null Values\n"," name           44\n","description     1\n","dtype: int64\n","Num Examples:  31548\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AGjcMQ_J9yXu"},"source":["Then, we read the test dataset:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pbCZSLQxVYr-","executionInfo":{"status":"ok","timestamp":1626950392682,"user_tz":-120,"elapsed":611,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"c6d913d0-d8df-4982-8b20-87fc36ea9f2d"},"source":["# Load the test dataset \n","test_df=pd.read_csv(testfile_path, header=0)\n","print('Num Examples: ',len(test_df))\n","print('Null Values\\n', test_df.isna().sum())\n","# there are no null values"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Num Examples:  1441\n","Null Values\n"," description    0\n","dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W_Djave6qwUA"},"source":["# Build a Tokenizer\n"]},{"cell_type":"markdown","metadata":{"id":"mv3X5LvKVtAN"},"source":["## Create the dataset to train a tokenizer\n","\n","*To train a tokenizer we need to save our dataset in a bunch of text files*. We create a plain text file for every description value and we will split each sample using a newline character. We include both the train and test dataset:"]},{"cell_type":"code","metadata":{"id":"FU9C8eNEc1jz"},"source":["# Drop the files from the output dir\n","txt_files_dir = \"./text_split\"\n","!rm -rf {txt_files_dir}\n","!mkdir {txt_files_dir}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GW5loLTdtUy"},"source":["# Store values in a dataframe column (Series object) to files, one file per record\n","def column_to_files(column, prefix, txt_files_dir):\n","    # The prefix is a unique ID to avoid to overwrite a text file\n","    i=prefix\n","    #For every value in the df, with just one column\n","    for row in column.to_list():\n","      # Create the filename using the prefix ID\n","      file_name = os.path.join(txt_files_dir, str(i)+'.txt')\n","      try:\n","        # Create the file and write the column text to it\n","        f = open(file_name, 'wb')\n","        f.write(row.encode('utf-8'))\n","        f.close()\n","      except Exception as e:  #catch exceptions(for eg. empty rows)\n","        print(row, e) \n","      i+=1\n","    # Return the last ID\n","    return i\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jqC5EASdrd8a"},"source":["Include the training dataset to the main text file:"]},{"cell_type":"code","metadata":{"id":"AurkxAD-Vx-M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626950908718,"user_tz":-120,"elapsed":1532,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"28341544-3e6c-4077-b0df-e51b0a7bbb70"},"source":["data = train_df[\"description\"]\n","# Removing the end of line character \\n\n","data = data.replace(\"\\n\",\" \")\n","# Set the ID to 0\n","prefix=0\n","# Create a file for every description value\n","prefix = column_to_files(data, prefix, txt_files_dir)\n","# Print the last ID\n","print(prefix)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["31548\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KrJTp3dRrkCX"},"source":["Also include the test dataset to the text file:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ldqNgudjd540","executionInfo":{"status":"ok","timestamp":1626950999689,"user_tz":-120,"elapsed":461,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"40098fb0-7f01-4b49-bc32-9c479bcb23f7"},"source":["data = test_df[\"description\"]\n","# Removing the end of line character \\n\n","data = data.replace(\"\\n\",\" \")\n","print(len(data))\n","# Create a file for every description value\n","prefix = column_to_files(data, prefix, txt_files_dir)\n","print(prefix)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1441\n","32989\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MPdhaKxTrxJx"},"source":["**Include the target variable for training** NOOOO¿?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7rxjGQJQox_y","executionInfo":{"status":"ok","timestamp":1614416233792,"user_tz":-60,"elapsed":1360,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"6b61da99-f021-4879-ab19-e7b77eea1446"},"source":["data = train_df[\"name\"]\n","data = data.replace(\"\\n\",\" \")\n","print(len(data))\n","prefix = column_to_files(data, prefix, txt_files_dir)\n","print(prefix)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["31548\n","64537\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G-kkz81OY6xH"},"source":["## Train the tokenizer\n","\n","The Stanford NLP group define the tokenization as:\n","\n","\"*Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.*\"\n","\n","A tokenizer breaks a string of characters, usually sentences of text, into tokens, an integer representation of the token, usually by looking for whitespace (tabs, spaces, new lines). It usually splits a sentence into words but there are many options like subwords.\n","\n","We will use a **byte-level Byte-pair encoding tokenizer**, byte pair encoding (BPE) is a simple form of data compression in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur within that data. The benefit of this method is that it will start building its vocabulary from an alphabet of single chars, so all words will be decomposable into tokens. We can avoid the presence of unknown (UNK) tokens.\n","\n","A great explanation on tokenizers can be found on the huggingface documentation, https://huggingface.co/transformers/tokenizer_summary.html."]},{"cell_type":"code","metadata":{"id":"5duRggBRZKvP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626951090879,"user_tz":-120,"elapsed":60341,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"b314c1f5-64fa-47f8-c8d8-d1b404c966a8"},"source":["# We won't need TensorFlow here\n","!pip uninstall -y tensorflow\n","# Install `transformers` from master\n","!pip install git+https://github.com/huggingface/transformers\n","!pip list | grep -E 'transformers|tokenizers'\n","# transformers version at notebook update --- 2.11.0\n","# tokenizers version at notebook update --- 0.8.0rc1\n","!pip install datasets==1.0.2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found existing installation: tensorflow 2.5.0\n","Uninstalling tensorflow-2.5.0:\n","  Successfully uninstalled tensorflow-2.5.0\n","Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-0d17qd9k\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-0d17qd9k\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0) (3.0.12)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 8.6 MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 37.0 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0) (4.41.1)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0) (4.6.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 50.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0) (21.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.9.0) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers==4.9.0) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.9.0) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.9.0) (3.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.9.0) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.9.0) (1.15.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.9.0-py3-none-any.whl size=2586796 sha256=fa49790febad35a144ba1ec22109332136f9fc5648610fb1c718f2ca5c3a10c8\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-_4pvwaae/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n","Successfully built transformers\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.0\n","tokenizers                    0.10.3\n","transformers                  4.9.0\n","Collecting datasets==1.0.2\n","  Downloading datasets-1.0.2-py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 6.9 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (1.1.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (4.41.1)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 57.8 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (0.3.4)\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (3.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (1.24.3)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.0.2) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.0.2) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.15.0)\n","Installing collected packages: xxhash, datasets\n","Successfully installed datasets-1.0.2 xxhash-2.0.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vYVIE1XcsV8_"},"source":["We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let’s pick its size to be 8,192 because our specific vocabulary is very limited and simple."]},{"cell_type":"code","metadata":{"id":"QgdLhEQS4c5f"},"source":["from pathlib import Path\n","\n","from tokenizers import ByteLevelBPETokenizer\n","from tokenizers.processors import BertProcessing\n","\n","from datasets import Dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CzKv-3nH66yq"},"source":["Now we can train our tokenizer on the text files containing our vocabulary, we need to specify the vocabulary size, the min frequency for a token to be included and the special tokens. We choose a vocab size of 8,192 and a min frequency of 2 (you can tune this value depending on your max vocabulary size). \n","\n","The special tokens depends on the model, for RoBERTa we include a short list: \n","- \\<s> or BOS, beginning Of Sentence\n","- \\</s> or EOS, End Of Sentence\n","- \\<pad> the padding token\n","- \\<unk> the unknown token\n","- \\<mask> the masking token."]},{"cell_type":"code","metadata":{"id":"IMnymRDLe0hi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626951263625,"user_tz":-120,"elapsed":1989,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"11fa5ca3-6a2a-44d9-9035-62415ecf6ee2"},"source":["%%time \n","paths = [str(x) for x in Path(\".\").glob(\"text_split/*.txt\")]\n","\n","# Initialize a tokenizer\n","tokenizer = ByteLevelBPETokenizer(lowercase=True)\n","\n","# Customize training\n","tokenizer.train(files=paths, vocab_size=8192, min_frequency=2,\n","                show_progress=True,\n","                special_tokens=[\n","                                \"<s>\",\n","                                \"<pad>\",\n","                                \"</s>\",\n","                                \"<unk>\",\n","                                \"<mask>\",\n","])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 2.19 s, sys: 885 ms, total: 3.07 s\n","Wall time: 1.75 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JBCOT9aI98fl","executionInfo":{"status":"ok","timestamp":1626951267547,"user_tz":-120,"elapsed":249,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"15971c25-f35b-4330-c252-43f7fa91368b"},"source":["tokenizer"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Tokenizer(vocabulary_size=8192, model=ByteLevelBPE, add_prefix_space=False, lowercase=True, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"6Ei7bqpRf1LH"},"source":["The count of samples is small and the tokenizer trains very fast. Now we can save the tokenizer to disk, later we will use it to train the language model:"]},{"cell_type":"code","metadata":{"id":"EIS-irI0f32P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626951312909,"user_tz":-120,"elapsed":1755,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"e1dd7cb0-c118-4d4a-bda3-9bfa203628e5"},"source":["#Save the Tokenizer to disk\n","tokenizer.save_model(tokenizer_folder)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/My Drive/Projects/text_generation_names/TokRoBERTa/vocab.json',\n"," '/content/drive/My Drive/Projects/text_generation_names/TokRoBERTa/merges.txt']"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"lOOfYSuQhSqT"},"source":["We now have both a `vocab.json`, which is a list of the most frequent tokens ranked by frequency and it is used to convert tokens to IDs, and a `merges.txt` file that maps texts to tokens.\n","\n","```json\n","{\n","\t\"<s>\": 0,\n","\t\"<pad>\": 1,\n","\t\"</s>\": 2,\n","\t\"<unk>\": 3,\n","\t\"<mask>\": 4,\n","\t\"!\": 5,\n","\t\"\\\"\": 6,\n","\t\"#\": 7,\n","\t\"$\": 8,\n","\t\"%\": 9,\n","\t\"&\": 10,\n","\t\"'\": 11,\n","\t\"(\": 12,\n","\t\")\": 13,\n","\t# ...\n","}\n","\n","# merges.txt\n","l a\n","Ġ k\n","o n\n","Ġ la\n","t a\n","Ġ e\n","Ġ d\n","Ġ p\n","# ...\n","```\n","\n","What is great is that our tokenizer is optimized for our very specific vocabulary. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. \n","\n","Here’s  how you can use it in `tokenizers`, including handling the RoBERTa special tokens – of course, you’ll also be able to use it directly from `transformers`. We can instantiate our tokenizer using both files and test it with some text from our dataset.\n"]},{"cell_type":"code","metadata":{"id":"tKVWB8WShT-z"},"source":["tokenizer = ByteLevelBPETokenizer(\n","    os.path.abspath(os.path.join(tokenizer_folder,'vocab.json')),\n","    os.path.abspath(os.path.join(tokenizer_folder,'merges.txt'))\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hO5M3vrAhcuj"},"source":["tokenizer._tokenizer.post_processor = BertProcessing(\n","    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",")\n","tokenizer.enable_truncation(max_length=512)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Z2IxmcK_eSf"},"source":["Let's show some examples:"]},{"cell_type":"code","metadata":{"id":"E3Ye27nchfzq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626951421564,"user_tz":-120,"elapsed":254,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"becc76ec-04aa-4095-cc3c-2847019bcf71"},"source":["tokenizer.encode(\"knit midi dress with vneckline straps.\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"X8ya5_7rhjKS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626951424185,"user_tz":-120,"elapsed":225,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"03fd3e62-553c-4f60-e507-89aa6fae385d"},"source":["tokenizer.encode(\"knit midi dress with vneckline straps.\").tokens"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s>',\n"," 'knit',\n"," 'Ġmidi',\n"," 'Ġdress',\n"," 'Ġwith',\n"," 'Ġvneckline',\n"," 'Ġstraps',\n"," '.',\n"," '</s>']"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"WQpUC_CDhnWW"},"source":["# Train a language model from scratch\n","\n","**Update:** This section follows along the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, using our new [`Trainer`](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) directly. Feel free to pick the approach you like best.\n","\n","> We’ll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the [documentation](https://huggingface.co/transformers/model_doc/roberta.html) for more details). In summary: *It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates* .\n","\n","As the model is BERT-like, we’ll train it on a task of **Masked language modeling**. It involves masking part of the input, about 10-20% of thre tokens, then learning a model to predict the missing tokens. MLM is often used within pretraining tasks, **to give models the opportunity to learn textual patterns from unlabeled data**. It can be fine tuned to a particular downstream task. The main benefit is that we do not need labeled data (hard to obtain), no text needs to be labeled by human labelers in order to predict the missing values.\n"]},{"cell_type":"markdown","metadata":{"id":"ylwOUseU3doR"},"source":["We define some global parameters:"]},{"cell_type":"code","metadata":{"id":"V6VsZnOd636F"},"source":["TRAIN_BATCH_SIZE = 16    # input batch size for training (default: 64)\n","VALID_BATCH_SIZE = 8    # input batch size for testing (default: 1000)\n","TRAIN_EPOCHS = 15        # number of epochs to train (default: 10)\n","LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n","SEED = 42               # random seed (default: 42)\n","MAX_LEN = 125\n","SUMMARY_LEN = 7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kD140sFjh0LQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626951474638,"user_tz":-120,"elapsed":257,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"1c41a858-6e74-46cd-df86-697531d0135d"},"source":["# Check that we have a GPU\n","!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Thu Jul 22 10:57:55 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VNZZs-r6iKAV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626951483457,"user_tz":-120,"elapsed":236,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"8b5f8775-a4a4-45c4-b4f5-a0c97a3b6642"},"source":["# Check that PyTorch sees it\n","import torch\n","torch.cuda.is_available()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"u0qQzgrBi1OX"},"source":["##Define the model\n","\n","We are going to train the model from scratch, not from a pretrained one. We create a model configuration for our RoBERTa model setting the main parameters:\n","- Vocabulary size\n","- Attention heads\n","- Hidden layers\n","- etc,"]},{"cell_type":"code","metadata":{"id":"LTXXutqeDzPi"},"source":["from transformers import RobertaConfig\n","\n","config = RobertaConfig(\n","    vocab_size=8192,\n","    max_position_embeddings=514,\n","    num_attention_heads=12,\n","    num_hidden_layers=6,\n","    type_vocab_size=1,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6yNCw-3hFv9h"},"source":["Finally let's initialize our model using the configuration file. As we are training from scratch, we only initialize from a config that define the architecture of the model but *not restoring previously trained weights*. The weights will be randomly initialized. "]},{"cell_type":"code","metadata":{"id":"BzMqR-dzF4Ro","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626951549390,"user_tz":-120,"elapsed":1508,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"2574701c-c62e-4445-93f8-f6fae2546949"},"source":["from transformers import RobertaForMaskedLM\n","\n","model = RobertaForMaskedLM(config=config)\n","print('Num parameters: ',model.num_parameters())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Num parameters:  49816064\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yAwQ82JiE5pi"},"source":["Now let's recreate our tokenizer, using the tokenizer trained and saved in the previous step. We will use a `RoBERTaTokenizerFast` object and the `from_pretrained` method, to initialize our tokenizer."]},{"cell_type":"code","metadata":{"id":"4keFBUjQFOD1"},"source":["from transformers import RobertaTokenizerFast\n","\n","tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, max_len=128)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RgXwwfgbG8WN","executionInfo":{"status":"ok","timestamp":1626951592848,"user_tz":-120,"elapsed":256,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"08851dbf-1892-412b-ead1-6ef90b8c98b6"},"source":["tokenizer"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PreTrainedTokenizerFast(name_or_path='/content/drive/My Drive/Projects/text_generation_names/TokRoBERTa', vocab_size=8192, model_max_len=128, is_fast=True, padding_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"jBtUHRMliOLM"},"source":["## Building the training Dataset\n","\n","We'll build our dataset by applying our tokenizer to our text file. As we did to train the tokenizer, we will save our training dataset to a disk file, one example per text line, and build the `Dataset` object using that file with the `LineByLineTextDataset` from the transformers library."]},{"cell_type":"code","metadata":{"id":"6YLQrEK8EoHC"},"source":["train_files_dir = \"./train_files\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PPcwSxp0AGHH"},"source":["Recreate the folder to contain the text files:"]},{"cell_type":"code","metadata":{"id":"AyR5ymHT_u3N"},"source":["!rm -rf {train_files_dir}\n","!mkdir {train_files_dir}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z4wIfHwwAXNW"},"source":["Create a text file that will contain every example in our training dataset:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0lSPCYCc_hbG","executionInfo":{"status":"ok","timestamp":1626951851705,"user_tz":-120,"elapsed":275,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"9eed1da3-0508-4bc7-da44-0622b581211a"},"source":["train_split = 1\n","train_data_size = int(len(train_df)*train_split)\n","test_data_size = int(len(test_df)*train_split)\n","print('Len Train data: ', str(train_data_size),' Len Test data: ', str(test_data_size))\n","\n","with open(os.path.join(train_files_dir,'train.txt') , 'w') as f:\n","    for item in train_df[:train_data_size].description.to_list():\n","        f.write(\"%s\\n\" % item)\n","    # We can evaluate to use the test file to train our language model\n","    #for item in test_df[:test_data_size].description.to_list():\n","    #    f.write(\"%s\\n\" % item)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Len Train data:  31548  Len Test data:  1441\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BJKYPraPDl0r"},"source":["## Create our dataset from the text file `train.txt`"]},{"cell_type":"code","metadata":{"id":"GlvP_A-THEEl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626953010652,"user_tz":-120,"elapsed":2236,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"8e37c2e7-83f0-4cab-9ebe-50f7a13e4883"},"source":["%%time\n","from transformers import LineByLineTextDataset\n","\n","dataset = LineByLineTextDataset(\n","    tokenizer=tokenizer,\n","    file_path=os.path.join(train_files_dir,'train.txt'),\n","    block_size=32,\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["CPU times: user 3.13 s, sys: 19.8 ms, total: 3.15 s\n","Wall time: 2.04 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Cm5cA1XUBkNB"},"source":["## Define the Data Collactor for masking our language"]},{"cell_type":"markdown","metadata":{"id":"hDLs73HcIHk5"},"source":["Like in the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, we need to define a data_collator.\n","\n","Once we have the dataset, a **Data Collator will helps us to mask our training texts**. This is just a small helper that will help us batch different samples of the dataset together into an object that PyTorch knows how to perform backprop on. Data collators are objects that will form a batch by using a list of dataset elements as input and may apply some processing like padding or random masking. The `DataCollatorForLanguageModeling` method allow us to set the probability with which to randomly mask tokens in the input."]},{"cell_type":"code","metadata":{"id":"zTgWPa9Dipk2"},"source":["from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZhOfZ-RByBr"},"source":["## Initialize and train our Trainer\n","\n","When we want to train a transformer model, the basic approach is to create a Trainer class that provides an API for feature-complete training and contains the basic training loop. First, we define the training arguments, there are many of them but the more relevant are\n","- `output_dir`, where the model artifacts will be saved\n","- `num_train_epochs`\n","- `per_device_train_batch_size`, the batch size\n","\n","\n"," and then the `Trainer` object is created with the arguments, the input dataset and the data collator defined:\n","\n"]},{"cell_type":"code","metadata":{"id":"YpvnFFmZJD-N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626954463564,"user_tz":-120,"elapsed":237,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"a9a37dc0-d5f9-4fd9-b236-875216e4b786"},"source":["from transformers import Trainer, TrainingArguments\n","\n","model_folder = os.path.abspath(os.path.join(root_folder, 'Projects/SpainAI NLP/DecRoBERTaML'))\n","print(model_folder)\n","\n","training_args = TrainingArguments(\n","    output_dir=model_folder,\n","    overwrite_output_dir=True,\n","    num_train_epochs=5,\n","    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n","    save_steps=8192,\n","    eval_steps=4096,\n","    save_total_limit=1,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset,\n","    #prediction_loss_only=True,\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/Projects/SpainAI NLP/DecRoBERTaML\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o6sASa36Nf-N"},"source":["And now, we are ready to train our model "]},{"cell_type":"code","metadata":{"id":"VmaHZXzmkNtJ","colab":{"base_uri":"https://localhost:8080/","height":980},"executionInfo":{"status":"ok","timestamp":1626955618996,"user_tz":-120,"elapsed":1118572,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"bdbaeda5-64e8-49d2-ab7a-233d83299434"},"source":["trainer.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["***** Running training *****\n","  Num examples = 32989\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 10310\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='10310' max='10310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [10310/10310 18:36, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>5.881400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>4.928700</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>4.462000</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>4.108900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>3.788700</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>3.541600</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>3.402600</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>3.225500</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>3.050500</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>3.056000</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>2.959400</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>2.860200</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>2.800200</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>2.746000</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>2.677500</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>2.622400</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>2.600300</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>2.475500</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>2.569800</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>2.528500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving model checkpoint to /content/drive/My Drive/Projects/SpainAI NLP/DecRoBERTaML/checkpoint-8192\n","Configuration saved in /content/drive/My Drive/Projects/SpainAI NLP/DecRoBERTaML/checkpoint-8192/config.json\n","Model weights saved in /content/drive/My Drive/Projects/SpainAI NLP/DecRoBERTaML/checkpoint-8192/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=10310, training_loss=3.288449064119711, metrics={'train_runtime': 1116.8467, 'train_samples_per_second': 147.688, 'train_steps_per_second': 9.231, 'total_flos': 2376871987329024.0, 'train_loss': 3.288449064119711, 'epoch': 5.0})"]},"metadata":{"tags":[]},"execution_count":89}]},{"cell_type":"markdown","metadata":{"id":"GegoobjKwfu8"},"source":["As a result, we can watch how the loss is decreasing while training."]},{"cell_type":"markdown","metadata":{"id":"_ZkooHz1-_2h"},"source":["## Save our final model and tokenizer to disk\n","\n","Save the model and tokenizer ina way that they can be restored for a future downstream task, our encoder decoder model"]},{"cell_type":"code","metadata":{"id":"QDNgPls7_l13"},"source":["trainer.save_model(model_folder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d0caceCy_p1-"},"source":["# Checking the trained model using a Pipeline"]},{"cell_type":"markdown","metadata":{"id":"iIQJ8ND_AEhl"},"source":["Looking at the training and eval losses going down is not enough, we would like to apply our model to check if our language model is learning anything interesting. An easy way is via the FillMaskPipeline.\n","\n","Pipelines are simple wrappers around tokenizers and models. **We can use the 'fill-mask' pipeline** where we input a sequence containing a masked token (<mask>) and it returns a list of the most probable filled sequences, with their probabilities.\n"]},{"cell_type":"code","metadata":{"id":"ltXgXyCbAJLY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613067164735,"user_tz":-60,"elapsed":3944,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"b835bb10-e797-4fe6-eb1f-ad8e1c8dccf0"},"source":["from transformers import pipeline\n","\n","fill_mask = pipeline(\n","    \"fill-mask\",\n","    model=os.path.abspath(os.path.join(output_folder,'TokRoBERTa')),\n","    tokenizer=os.path.abspath(os.path.join(output_folder,'TokRoBERTa'))\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/My Drive/Projects/SpainAI NLP/TokRoBERTa and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"UIvgZ3S6AO0z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613067167134,"user_tz":-60,"elapsed":723,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"7075b8eb-66cc-4cb3-99db-4769e43ea9a7"},"source":["# knit midi dress with vneckline\n","# =>\n","fill_mask(\"midi <mask> with vneckline.\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.37433964014053345,\n","  'sequence': 'midi dress with vneckline.',\n","  'token': 482,\n","  'token_str': ' dress'},\n"," {'score': 0.33222395181655884,\n","  'sequence': 'midi skirt with vneckline.',\n","  'token': 769,\n","  'token_str': ' skirt'},\n"," {'score': 0.035536717623472214,\n","  'sequence': 'midi crop with vneckline.',\n","  'token': 1693,\n","  'token_str': ' crop'},\n"," {'score': 0.023702150210738182,\n","  'sequence': 'midi sleeve with vneckline.',\n","  'token': 469,\n","  'token_str': ' sleeve'},\n"," {'score': 0.0199593435972929,\n","  'sequence': 'midi vest with vneckline.',\n","  'token': 2315,\n","  'token_str': ' vest'}]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"i0qCyyhNAWZi"},"source":["Ok, simple syntax/grammar works. Let’s try a slightly more interesting prompt:\n","\n"]},{"cell_type":"code","metadata":{"id":"YZ9HSQxAAbme","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"aabfeedc-b1d0-4837-b01d-cd42726a5a3d"},"source":["# The test text: Round neck sweater with long sleeves\n","fill_mask(\"Round neck sweater with <mask> sleeves.\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.01814725436270237,\n","  'sequence': '<s> Jen la komenco de bela urbo.</s>',\n","  'token': 871},\n"," {'score': 0.015888698399066925,\n","  'sequence': '<s> Jen la komenco de bela vivo.</s>',\n","  'token': 1160},\n"," {'score': 0.015662025660276413,\n","  'sequence': '<s> Jen la komenco de bela tempo.</s>',\n","  'token': 1021},\n"," {'score': 0.015555007383227348,\n","  'sequence': '<s> Jen la komenco de bela mondo.</s>',\n","  'token': 945},\n"," {'score': 0.01412549614906311,\n","  'sequence': '<s> Jen la komenco de bela tago.</s>',\n","  'token': 1633}]"]},"metadata":{"tags":[]},"execution_count":37}]}]}