Let's test our model!

print(val_enc_inputs[0][1:16])
print(val_dec_outputs[0][1:5])

output = predict(tokenizer.decode(val_enc_inputs[0][1:16]))
print('Target output: ',tokenizer.decode(val_dec_outputs[0][1:5]))

#input_data[1][3:]
#[2]+[3,4]+[5]
tokenizer.index_word[64]

#sentence = 'printed bermuda shorts made technical fabric . elastic drawstring waistband . front pockets rear pocket detail . patch pockets with fastening legs.'

#sentence = sos_token_input+tokenizer.texts_to_sequences([sentence])+eos_token_input
#print(sentence)
s=tokenizer.texts_to_sequences(['<start> '+'printed bermuda shorts made technical fabric . elastic drawstring waistband . front pockets rear pocket detail . patch pockets with fastening legs.'+' <end>'])
tf.expand_dims(s,0)
#evaluate_word_token('printed bermuda shorts made technical fabric . elastic drawstring waistband . front pockets rear pocket detail . patch pockets with fastening legs.')

sentence = 'printed bermuda shorts made technical fabric . elastic drawstring waistband . front pockets rear pocket detail . patch pockets with fastening legs .'
input=[]
for i in sentence.split(' '):
    if i in tokenizer.word_index:
        input.append(tokenizer.word_index[i] if tokenizer.word_index[i] < VOCAB_SIZE 
            else tokenizer.word_index['<unk>'])
input = pad_sequences([input], maxlen=TEXT_MAX_LENGTH, truncating='post',padding='post')
input = tf.convert_to_tensor(input)
#input = tf.expand_dims(input, 0)
print(input)

'cable' in tokenizer.word_index

#s=evaluate_word_token('printed bermuda shorts made technical fabric . elastic drawstring waistband . front pockets rear pocket detail . patch pockets with fastening legs.')
predict('printed bermuda shorts made technical fabric . elastic drawstring waistband . front pockets rear pocket detail . patch pockets with fastening legs.')

