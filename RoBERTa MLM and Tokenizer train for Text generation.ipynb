{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBERTa MLM and Tokenizer train for Text generation.ipynb","provenance":[{"file_id":"https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb","timestamp":1612196531906}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"M1oqh0F6W3ad"},"source":["# Train a language model (Masked Language Modelling) from scratch using Huggingface Transformers and a custom tokenizer\n","\n","### Inspired from the great notebook by Huggingfce (link to blogpost [link](https://huggingface.co/blog/how-to-train)).\n","\n","# Brief Introduction\n","This blog post is the first part of a series where we want to create a product names generator using a transformer model. For a few weeks I was investigating different models and alternatives in Huggingface to train a text generation model. We have a short list of products with their description and our goal is to obtain the name of the product. I did some experiments with the Transformer model in Tensorflow as well as the T5 summarizer. Finally, in order to deepen the use of Huggingface transformers, I decided to approach the problem with a somewhat more complex approach, an encoder decoder model. Maybe it was not the best option but I wanted to learn new things about huggingface Transformers. In the next post of the series we will introduce you deeper in this concept.\n","\n","Here, in this first part, we will show how to train a tokenizer from scratch and how to use Masked Language Modeling technique to create a RoBERTa model. This personalized model will become the base model for our future encoder Decoder model.\n","\n","# Our Solution\n","For our experiment we are going to train from scratch a RoBERTa model, it will become the encoder and the decoder of a future model. But our domain is very specific, words and concepts about clothes, shapes, colors, … Therefore, we are interested in defining our own tokenizer created from our specific vocabulary, avoiding to include more common words from other domains or use cases which are irrelevant for our final purpose.\n","\n","*We can describe our training phase in three main steps*:\n","- Create and train a byte-level, **Byte-pair encoding tokenizer** with the same special tokens as RoBERTa\n","- Train a RoBERTa model from scratch using **Masked Language Modeling**, MLM.\n","- Warm start and **fine tune an encoder decoder model** based on our RoBERTa pretrained model.\n","\n","In this post we’ll demo how to train a “small” RoBERTa model (6 layers, 768 hidden size, 12 attention heads) – that’s the same number of layers & heads as DistilBERT – on a language from a clothing shop. And in a next notebook, we’ll then fine-tune the model on a downstream task of text generation.\n"]},{"cell_type":"markdown","metadata":{"id":"flij-vszUsQm"},"source":["# Loading the libraries"]},{"cell_type":"code","metadata":{"id":"mc6KQfWIUuZB","executionInfo":{"status":"ok","timestamp":1632650661606,"user_tz":-120,"elapsed":3,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}}},"source":["import os\n","import pandas as pd\n","import tqdm\n","import math"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tvVMVDYjUaRt"},"source":["# Loading the datasets\n","\n","As we mentioned before, our dataset contains around 31.000 items, about clothes from an important retailer, including a long product description and a short product name, our target variable. First, we execute a exploratory data analysis and we can observe that the count of rows with outliers values is a small number. The count of words looks like a left skewed distribution, 75% of rows in the range 50–60 words and a maximum about 125 words. The target variable contains about 3 to 6 words."]},{"cell_type":"code","metadata":{"id":"HOk4iZ9YZvec","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632650696042,"user_tz":-120,"elapsed":23368,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"26cf150b-0028-4ec4-9d08-9a6787246656"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"5TwDqHbL9SY0"},"source":["Set the variables to the data folders:"]},{"cell_type":"code","metadata":{"id":"f2tjpFnyU-8T","executionInfo":{"status":"ok","timestamp":1632650711388,"user_tz":-120,"elapsed":169,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}}},"source":["#Set the path to the data folder, datafile and output folder and files\n","root_folder = '/content/drive/My Drive/'\n","data_folder = os.path.abspath(os.path.join(root_folder, 'datasets/text_gen_product_names'))\n","model_folder = os.path.abspath(os.path.join(root_folder, 'Projects/text_generation_names/RoBERTaMLM'))\n","output_folder = os.path.abspath(os.path.join(root_folder, 'Projects/text_generation_names'))\n","tokenizer_folder = os.path.abspath(os.path.join(root_folder, 'Projects/text_generation_names/TokRoBERTa'))\n","\n","test_filename='cl_test_descriptions.csv'\n","datafile= 'product_names_desc_cl_train.csv'\n","outputfile = 'submission.csv'\n","\n","datafile_path = os.path.abspath(os.path.join(data_folder,datafile))\n","testfile_path = os.path.abspath(os.path.join(data_folder,test_filename))\n","outputfile_path = os.path.abspath(os.path.join(output_folder,outputfile))"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nx_diDArVIHa"},"source":["Load the train datafile with the product descriptions and names:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MqxDq-LcVL0T","executionInfo":{"status":"ok","timestamp":1632650715526,"user_tz":-120,"elapsed":1299,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"136865dd-4164-4656-8b6b-da15d27cda78"},"source":["# Load the train dataset\n","train_df=pd.read_csv(datafile_path, header=0, usecols=[0,1])\n","# Show the count of rows\n","print('Num Examples: ',len(train_df))\n","print('Null Values\\n', train_df.isna().sum())\n","# Drop rows with Null values \n","train_df.dropna(inplace=True)\n","print('Num Examples: ',len(train_df))"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Num Examples:  31593\n","Null Values\n"," name           44\n","description     1\n","dtype: int64\n","Num Examples:  31548\n"]}]},{"cell_type":"markdown","metadata":{"id":"AGjcMQ_J9yXu"},"source":["Then, we read the test dataset:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pbCZSLQxVYr-","executionInfo":{"status":"ok","timestamp":1632650719313,"user_tz":-120,"elapsed":523,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"55b1429b-11b2-4789-96c0-b97ab3e11abf"},"source":["# Load the test dataset \n","test_df=pd.read_csv(testfile_path, header=0)\n","print('Num Examples: ',len(test_df))\n","print('Null Values\\n', test_df.isna().sum())\n","# there are no null values"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Num Examples:  1441\n","Null Values\n"," description    0\n","dtype: int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"W_Djave6qwUA"},"source":["# Build a Tokenizer\n"]},{"cell_type":"markdown","metadata":{"id":"mv3X5LvKVtAN"},"source":["## Create the dataset to train a tokenizer\n","\n","*To train a tokenizer we need to save our dataset in a bunch of text files*. We create a plain text file for every description value and we will split each sample using a newline character. We include both the train and test dataset:"]},{"cell_type":"code","metadata":{"id":"FU9C8eNEc1jz","executionInfo":{"status":"ok","timestamp":1632650723529,"user_tz":-120,"elapsed":484,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}}},"source":["# Drop the files from the output dir\n","txt_files_dir = \"./text_split\"\n","!rm -rf {txt_files_dir}\n","!mkdir {txt_files_dir}"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GW5loLTdtUy","executionInfo":{"status":"ok","timestamp":1632650726368,"user_tz":-120,"elapsed":187,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}}},"source":["# Store values in a dataframe column (Series object) to files, one file per record\n","def column_to_files(column, prefix, txt_files_dir):\n","    # The prefix is a unique ID to avoid to overwrite a text file\n","    i=prefix\n","    #For every value in the df, with just one column\n","    for row in column.to_list():\n","      # Create the filename using the prefix ID\n","      file_name = os.path.join(txt_files_dir, str(i)+'.txt')\n","      try:\n","        # Create the file and write the column text to it\n","        f = open(file_name, 'wb')\n","        f.write(row.encode('utf-8'))\n","        f.close()\n","      except Exception as e:  #catch exceptions(for eg. empty rows)\n","        print(row, e) \n","      i+=1\n","    # Return the last ID\n","    return i\n"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jqC5EASdrd8a"},"source":["Include the training dataset to the main text file:"]},{"cell_type":"code","metadata":{"id":"AurkxAD-Vx-M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632650732737,"user_tz":-120,"elapsed":2172,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"ba03eb57-8e36-4f78-955e-299717f5fbe9"},"source":["data = train_df[\"description\"]\n","# Removing the end of line character \\n\n","data = data.replace(\"\\n\",\" \")\n","# Set the ID to 0\n","prefix=0\n","# Create a file for every description value\n","prefix = column_to_files(data, prefix, txt_files_dir)\n","# Print the last ID\n","print(prefix)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["31548\n"]}]},{"cell_type":"markdown","metadata":{"id":"KrJTp3dRrkCX"},"source":["Also include the test dataset to the text file:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ldqNgudjd540","executionInfo":{"status":"ok","timestamp":1632650736439,"user_tz":-120,"elapsed":190,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"8c8130ca-35a5-4c59-89db-9e3cf1a022c8"},"source":["data = test_df[\"description\"]\n","# Removing the end of line character \\n\n","data = data.replace(\"\\n\",\" \")\n","print(len(data))\n","# Create a file for every description value\n","prefix = column_to_files(data, prefix, txt_files_dir)\n","print(prefix)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["1441\n","32989\n"]}]},{"cell_type":"markdown","metadata":{"id":"MPdhaKxTrxJx"},"source":["We will include the products name to our tokenizer vocabulary in order to learn about it."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7rxjGQJQox_y","executionInfo":{"status":"ok","timestamp":1632650908747,"user_tz":-120,"elapsed":2074,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"e85ede97-487d-40a6-8cac-104e905ed705"},"source":["data = train_df[\"name\"]\n","data = data.replace(\"\\n\",\" \")\n","print(len(data))\n","prefix = column_to_files(data, prefix, txt_files_dir)\n","print(prefix)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["31548\n","64537\n"]}]},{"cell_type":"markdown","metadata":{"id":"G-kkz81OY6xH"},"source":["## Train the tokenizer\n","\n","The Stanford NLP group define the tokenization as:\n","\n","\"*Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.*\"\n","\n","A tokenizer breaks a string of characters, usually sentences of text, into tokens, an integer representation of the token, usually by looking for whitespace (tabs, spaces, new lines). It usually splits a sentence into words but there are many options like subwords.\n","\n","We will use a **byte-level Byte-pair encoding tokenizer**, byte pair encoding (BPE) is a simple form of data compression in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur within that data. The benefit of this method is that it will start building its vocabulary from an alphabet of single chars, so all words will be decomposable into tokens. We can avoid the presence of unknown (UNK) tokens.\n","\n","A great explanation on tokenizers can be found on the huggingface documentation, https://huggingface.co/transformers/tokenizer_summary.html."]},{"cell_type":"code","metadata":{"id":"5duRggBRZKvP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632650982866,"user_tz":-120,"elapsed":69195,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"fe71e582-31d0-4b8f-ccc4-caadad0920e6"},"source":["# We won't need TensorFlow here\n","!pip uninstall -y tensorflow\n","# Install `transformers` from master\n","!pip install git+https://github.com/huggingface/transformers\n","!pip list | grep -E 'transformers|tokenizers'\n","# transformers version at notebook update --- 2.11.0\n","# tokenizers version at notebook update --- 0.8.0rc1\n","!pip install datasets==1.0.2"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.6.0\n","Uninstalling tensorflow-2.6.0:\n","  Successfully uninstalled tensorflow-2.6.0\n","Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-iljlcupa\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-iljlcupa\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting huggingface-hub>=0.0.17\n","  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 1.2 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 9.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (4.62.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (3.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (1.19.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 39.9 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (2019.12.20)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 46.4 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (4.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.0.dev0) (21.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers==4.11.0.dev0) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.11.0.dev0) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.11.0.dev0) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.0.dev0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.0.dev0) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.0.dev0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.0.dev0) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.0.dev0) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.0.dev0) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.0.dev0) (1.15.0)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.11.0.dev0-py3-none-any.whl size=2870194 sha256=514f386f8784ac4e86e79ca6366d13144788d410c8d4d90c18aa4eef309b3f94\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-zg_2s5o5/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n","Successfully built transformers\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.0.dev0\n","tokenizers                    0.10.3\n","transformers                  4.11.0.dev0\n","Collecting datasets==1.0.2\n","  Downloading datasets-1.0.2-py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (3.0.0)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 48.0 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (0.3.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (4.62.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (1.19.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (1.1.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (1.24.3)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.0.2) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.0.2) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.15.0)\n","Installing collected packages: xxhash, datasets\n","Successfully installed datasets-1.0.2 xxhash-2.0.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"vYVIE1XcsV8_"},"source":["We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let’s pick its size to be 8,192 because our specific vocabulary is very limited and simple."]},{"cell_type":"code","metadata":{"id":"QgdLhEQS4c5f","executionInfo":{"status":"ok","timestamp":1632650992956,"user_tz":-120,"elapsed":3765,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}}},"source":["from pathlib import Path\n","\n","from tokenizers import ByteLevelBPETokenizer\n","from tokenizers.processors import BertProcessing\n","\n","import torch\n","from torch.utils.data.dataset import Dataset"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CzKv-3nH66yq"},"source":["Now we can train our tokenizer on the text files containing our vocabulary, we need to specify the vocabulary size, the min frequency for a token to be included and the special tokens. We choose a vocab size of 8,192 and a min frequency of 2 (you can tune this value depending on your max vocabulary size). \n","\n","The special tokens depends on the model, for RoBERTa we include a short list: \n","- \\<s> or BOS, beginning Of Sentence\n","- \\</s> or EOS, End Of Sentence\n","- \\<pad> the padding token\n","- \\<unk> the unknown token\n","- \\<mask> the masking token."]},{"cell_type":"code","metadata":{"id":"IMnymRDLe0hi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632651001510,"user_tz":-120,"elapsed":3813,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"d58be186-3ce2-406d-c1f0-7b86713f3dab"},"source":["%%time \n","paths = [str(x) for x in Path(\".\").glob(\"text_split/*.txt\")]\n","\n","# Initialize a tokenizer\n","tokenizer = ByteLevelBPETokenizer(lowercase=True)\n","\n","# Customize training\n","tokenizer.train(files=paths, vocab_size=8192, min_frequency=2,\n","                show_progress=True,\n","                special_tokens=[\n","                                \"<s>\",\n","                                \"<pad>\",\n","                                \"</s>\",\n","                                \"<unk>\",\n","                                \"<mask>\",\n","])"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 3.74 s, sys: 2.41 s, total: 6.15 s\n","Wall time: 3.52 s\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JBCOT9aI98fl","executionInfo":{"status":"ok","timestamp":1632651005232,"user_tz":-120,"elapsed":380,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"7f68a626-1aab-45da-d58a-e8f904ff27f4"},"source":["tokenizer"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Tokenizer(vocabulary_size=8192, model=ByteLevelBPE, add_prefix_space=False, lowercase=True, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"6Ei7bqpRf1LH"},"source":["The count of samples is small and the tokenizer trains very fast. Now we can save the tokenizer to disk, later we will use it to train the language model:"]},{"cell_type":"code","metadata":{"id":"EIS-irI0f32P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632651011364,"user_tz":-120,"elapsed":221,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"3a7be772-7a23-4f11-8ebf-7f085cb1ed8f"},"source":["#Save the Tokenizer to disk\n","tokenizer.save_model(tokenizer_folder)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/My Drive/Projects/text_generation_names/TokRoBERTa/vocab.json',\n"," '/content/drive/My Drive/Projects/text_generation_names/TokRoBERTa/merges.txt']"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"lOOfYSuQhSqT"},"source":["We now have both a `vocab.json`, which is a list of the most frequent tokens ranked by frequency and it is used to convert tokens to IDs, and a `merges.txt` file that maps texts to tokens.\n","\n","```json\n","{\n","\t\"<s>\": 0,\n","\t\"<pad>\": 1,\n","\t\"</s>\": 2,\n","\t\"<unk>\": 3,\n","\t\"<mask>\": 4,\n","\t\"!\": 5,\n","\t\"\\\"\": 6,\n","\t\"#\": 7,\n","\t\"$\": 8,\n","\t\"%\": 9,\n","\t\"&\": 10,\n","\t\"'\": 11,\n","\t\"(\": 12,\n","\t\")\": 13,\n","\t# ...\n","}\n","\n","# merges.txt\n","l a\n","Ġ k\n","o n\n","Ġ la\n","t a\n","Ġ e\n","Ġ d\n","Ġ p\n","# ...\n","```\n","\n","What is great is that our tokenizer is optimized for our very specific vocabulary. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. \n","\n","Here’s  how you can use it in `tokenizers`, including handling the RoBERTa special tokens – of course, you’ll also be able to use it directly from `transformers`. We can instantiate our tokenizer using both files and test it with some text from our dataset.\n"]},{"cell_type":"code","metadata":{"id":"tKVWB8WShT-z","executionInfo":{"status":"ok","timestamp":1632651016840,"user_tz":-120,"elapsed":182,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}}},"source":["# Create the tokenizer using vocab.json and mrege.txt files\n","tokenizer = ByteLevelBPETokenizer(\n","    os.path.abspath(os.path.join(tokenizer_folder,'vocab.json')),\n","    os.path.abspath(os.path.join(tokenizer_folder,'merges.txt'))\n",")"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"hO5M3vrAhcuj","executionInfo":{"status":"ok","timestamp":1632651018921,"user_tz":-120,"elapsed":197,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}}},"source":["# Prepare the tokenizer\n","tokenizer._tokenizer.post_processor = BertProcessing(\n","    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",")\n","tokenizer.enable_truncation(max_length=512)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Z2IxmcK_eSf"},"source":["Let's show some examples:"]},{"cell_type":"code","metadata":{"id":"E3Ye27nchfzq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632651021927,"user_tz":-120,"elapsed":180,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"3a1ccd24-8def-4d48-e90d-bcd9b9c11f62"},"source":["tokenizer.encode(\"knit midi dress with vneckline straps.\")"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"X8ya5_7rhjKS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632651025782,"user_tz":-120,"elapsed":167,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"62557631-efd6-4a3a-a070-1b1c6b914d9a"},"source":["# Show the tokens created\n","tokenizer.encode(\"knit midi dress with vneckline straps.\").tokens"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s>',\n"," 'knit',\n"," 'Ġmidi',\n"," 'Ġdress',\n"," 'Ġwith',\n"," 'Ġvneckline',\n"," 'Ġstraps',\n"," '.',\n"," '</s>']"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"WQpUC_CDhnWW"},"source":["# Train a language model from scratch\n","\n","**Update:** This section follows along the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, using our new [`Trainer`](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) directly. Feel free to pick the approach you like best.\n","\n","> We’ll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the [documentation](https://huggingface.co/transformers/model_doc/roberta.html) for more details). In summary: *It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates* .\n","\n","As the model is BERT-like, we’ll train it on a task of **Masked language modeling**. It involves masking part of the input, about 10-20% of thre tokens, then learning a model to predict the missing tokens. MLM is often used within pretraining tasks, **to give models the opportunity to learn textual patterns from unlabeled data**. It can be fine tuned to a particular downstream task. The main benefit is that we do not need labeled data (hard to obtain), no text needs to be labeled by human labelers in order to predict the missing values.\n"]},{"cell_type":"markdown","metadata":{"id":"ylwOUseU3doR"},"source":["We define some global parameters:"]},{"cell_type":"code","metadata":{"id":"V6VsZnOd636F","executionInfo":{"status":"ok","timestamp":1632651032275,"user_tz":-120,"elapsed":289,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}}},"source":["TRAIN_BATCH_SIZE = 16    # input batch size for training (default: 64)\n","VALID_BATCH_SIZE = 8    # input batch size for testing (default: 1000)\n","TRAIN_EPOCHS = 15        # number of epochs to train (default: 10)\n","LEARNING_RATE = 1e-4    # learning rate (default: 0.001)\n","WEIGHT_DECAY = 0.01\n","SEED = 42               # random seed (default: 42)\n","MAX_LEN = 128\n","SUMMARY_LEN = 7"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"kD140sFjh0LQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632651035258,"user_tz":-120,"elapsed":206,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"aec0051b-83ea-48e8-e364-b08a36722b0e"},"source":["# Check that we have a GPU\n","!nvidia-smi"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Sep 26 10:10:36 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   61C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"VNZZs-r6iKAV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632651038039,"user_tz":-120,"elapsed":279,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"61a65b18-047b-4fe6-e2bd-6ed088d76094"},"source":["# Check that PyTorch sees it\n","import torch\n","torch.cuda.is_available()"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"u0qQzgrBi1OX"},"source":["##Define the model\n","\n","We are going to train the model from scratch, not from a pretrained one. We create a model configuration for our RoBERTa model setting the main parameters:\n","- Vocabulary size\n","- Attention heads\n","- Hidden layers\n","- etc,"]},{"cell_type":"code","metadata":{"id":"LTXXutqeDzPi","executionInfo":{"status":"ok","timestamp":1632651041857,"user_tz":-120,"elapsed":513,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}}},"source":["from transformers import RobertaConfig\n","\n","config = RobertaConfig(\n","    vocab_size=8192,\n","    max_position_embeddings=514,\n","    num_attention_heads=12,\n","    num_hidden_layers=6,\n","    type_vocab_size=1,\n",")"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6yNCw-3hFv9h"},"source":["Finally let's initialize our model using the configuration file. As we are training from scratch, we only initialize from a config that define the architecture of the model but *not restoring previously trained weights*. The weights will be randomly initialized. "]},{"cell_type":"code","metadata":{"id":"BzMqR-dzF4Ro","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632651066059,"user_tz":-120,"elapsed":1821,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"1e9ef7cd-3fa1-46ae-c844-72d2f57fe22e"},"source":["from transformers import RobertaForMaskedLM\n","\n","model = RobertaForMaskedLM(config=config)\n","print('Num parameters: ',model.num_parameters())"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Num parameters:  49816064\n"]}]},{"cell_type":"markdown","metadata":{"id":"yAwQ82JiE5pi"},"source":["Now let's recreate our tokenizer, using the tokenizer trained and saved in the previous step. We will use a `RoBERTaTokenizerFast` object and the `from_pretrained` method, to initialize our tokenizer."]},{"cell_type":"code","metadata":{"id":"4keFBUjQFOD1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632651071276,"user_tz":-120,"elapsed":495,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"63491299-ddfe-43e2-868a-01a2f5073a5c"},"source":["from transformers import RobertaTokenizerFast\n","# Create the tokenizer from a trained one\n","tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, max_len=MAX_LEN)"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n","  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RgXwwfgbG8WN","executionInfo":{"status":"ok","timestamp":1632651073256,"user_tz":-120,"elapsed":14,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"eaaa2ce5-68a6-4fad-990c-e303e8adfabd"},"source":["tokenizer"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PreTrainedTokenizerFast(name_or_path='/content/drive/My Drive/Projects/text_generation_names/TokRoBERTa', vocab_size=8192, model_max_len=128, is_fast=True, padding_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"jBtUHRMliOLM"},"source":["## Building the training Dataset\n","\n","We'll build a Pytorch dataset, subclassing the Dataset Class. The CustomDataset receives a Pandas Series with the `description` variable values and the tokenizer to encode those values. The Dataset returns a list of tokens for every product description in the Series.\n","\n","In order to evaluate the model during training, we will generate a train dataset for training and a evaluation dataset.\n"]},{"cell_type":"markdown","metadata":{"id":"XBdFLR-dZnXs"},"source":["Example of using a custom Dataset: https://ryanong.co.uk/2020/06/11/day-163-how-to-build-a-language-model-from-scratch-implementation/"]},{"cell_type":"code","metadata":{"id":"WFMmfy_LDpRZ","executionInfo":{"status":"ok","timestamp":1632651104720,"user_tz":-120,"elapsed":211,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}}},"source":["class CustomDataset(Dataset):\n","    def __init__(self, df, tokenizer):\n","        # or use the RobertaTokenizer from `transformers` directly.\n","\n","        self.examples = []\n","        \n","        for example in df.values:\n","            x=tokenizer.encode_plus(example, max_length = MAX_LEN, truncation=True, padding=True)\n","            self.examples += [x.input_ids]\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, i):\n","        # We’ll pad at the batch level.\n","        return torch.tensor(self.examples[i])"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q4Ovn-MTm149"},"source":["Concat the training and test dataset, only with the description column."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0miQ7KOQ3eHp","executionInfo":{"status":"ok","timestamp":1626954424780,"user_tz":-120,"elapsed":258,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"e5a01677-018b-4c87-a002-e0bde19d8779"},"source":["# Concatenate the train dataset and the test dataset for language modelling\n","#df=pd.concat([train_df['description'], test_df['description']], axis=0)\n","#print('Total: ',len(df), len(train_df), len(test_df))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["32989 31548 1441\n","0       towel with border with lines metallic thread .\n","1    printed bermuda shorts made technical fabric ....\n","2    bodysuit with shapewear effect . this bodysuit...\n","3    vneck with thin adjustable straps.height model...\n","4    puritan collar dress featuring long sleeves wi...\n","Name: description, dtype: object\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ACOfK4avA_qx"},"source":["Create the custom datasets, for training and evaluation:"]},{"cell_type":"code","metadata":{"id":"5aTvI2sBDK5N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632651133912,"user_tz":-120,"elapsed":9034,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"b228af16-26f0-46d8-8ca4-e339cc565aa0"},"source":["# Create the train and evaluation dataset\n","train_dataset = CustomDataset(train_df['description'], tokenizer)\n","eval_dataset = CustomDataset(test_df['description'], tokenizer)"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2227: UserWarning: `max_length` is ignored when `padding`=`True`.\n","  warnings.warn(\"`max_length` is ignored when `padding`=`True`.\")\n"]}]},{"cell_type":"markdown","metadata":{"id":"Cm5cA1XUBkNB"},"source":["## Define the Data Collactor for masking our language"]},{"cell_type":"markdown","metadata":{"id":"hDLs73HcIHk5"},"source":["Like in the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, we need to define a data_collator.\n","\n","Once we have the dataset, a **Data Collator will helps us to mask our training texts**. This is just a small helper that will help us batch different samples of the dataset together into an object that PyTorch knows how to perform backprop on. Data collators are objects that will form a batch by using a list of dataset elements as input and may apply some processing like padding or random masking. The `DataCollatorForLanguageModeling` method allow us to set the probability with which to randomly mask tokens in the input."]},{"cell_type":"code","metadata":{"id":"zTgWPa9Dipk2","executionInfo":{"status":"ok","timestamp":1632651148289,"user_tz":-120,"elapsed":925,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}}},"source":["from transformers import DataCollatorForLanguageModeling\n","\n","# Define the Data Collator\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZhOfZ-RByBr"},"source":["## Initialize and train our Trainer\n","\n","When we want to train a transformer model, the basic approach is to create a Trainer class that provides an API for feature-complete training and contains the basic training loop. First, we define the training arguments, there are many of them but the more relevant are\n","- `output_dir`, where the model artifacts will be saved\n","- `num_train_epochs`\n","- `per_device_train_batch_size`, the batch size\n","\n","\n"," and then the `Trainer` object is created with the arguments, the input dataset and the data collator defined:\n","\n"]},{"cell_type":"code","metadata":{"id":"YpvnFFmZJD-N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632651187774,"user_tz":-120,"elapsed":9791,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"f1bed3de-9283-4504-d3e3-a68a28a3dfea"},"source":["from transformers import Trainer, TrainingArguments\n","\n","print(model_folder)\n","# Define the training arguments\n","training_args = TrainingArguments(\n","    output_dir=model_folder,\n","    overwrite_output_dir=True,\n","    evaluation_strategy = 'epoch',\n","    num_train_epochs=TRAIN_EPOCHS,\n","    learning_rate=LEARNING_RATE,\n","    weight_decay=WEIGHT_DECAY,\n","    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n","    per_device_eval_batch_size=VALID_BATCH_SIZE,\n","    save_steps=8192,\n","    #eval_steps=4096,\n","    save_total_limit=1,\n",")\n","# Create the trainer for our model\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    #prediction_loss_only=True,\n",")"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM\n"]}]},{"cell_type":"markdown","metadata":{"id":"o6sASa36Nf-N"},"source":["And now, we are ready to train our model "]},{"cell_type":"code","metadata":{"id":"VmaHZXzmkNtJ","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1632657428676,"user_tz":-120,"elapsed":6231393,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"212df16e-4f20-42b8-d564-8a929d59d30c"},"source":["# Train the model\n","trainer.train()"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 31548\n","  Num Epochs = 15\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 29580\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='29580' max='29580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [29580/29580 1:43:49, Epoch 15/15]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>4.347100</td>\n","      <td>4.933113</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.158300</td>\n","      <td>4.069774</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>2.653300</td>\n","      <td>3.661828</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>2.397400</td>\n","      <td>3.423693</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>2.166700</td>\n","      <td>3.137322</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>2.030000</td>\n","      <td>2.955188</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>1.917700</td>\n","      <td>2.957569</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>1.820700</td>\n","      <td>2.667487</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>1.730600</td>\n","      <td>2.732139</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>1.618200</td>\n","      <td>2.612350</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>1.582600</td>\n","      <td>2.485756</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>1.491100</td>\n","      <td>2.470491</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>1.475600</td>\n","      <td>2.474639</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>1.449500</td>\n","      <td>2.437338</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>1.375400</td>\n","      <td>2.361527</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/checkpoint-8192\n","Configuration saved in /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/checkpoint-8192/config.json\n","Model weights saved in /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/checkpoint-8192/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/checkpoint-24576] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/checkpoint-16384\n","Configuration saved in /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/checkpoint-16384/config.json\n","Model weights saved in /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/checkpoint-16384/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/checkpoint-8192] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","Saving model checkpoint to /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/checkpoint-24576\n","Configuration saved in /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/checkpoint-24576/config.json\n","Model weights saved in /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/checkpoint-24576/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/checkpoint-16384] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=29580, training_loss=2.1408384223819343, metrics={'train_runtime': 6229.9254, 'train_samples_per_second': 75.959, 'train_steps_per_second': 4.748, 'total_flos': 6611108500359168.0, 'train_loss': 2.1408384223819343, 'epoch': 15.0})"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"GegoobjKwfu8"},"source":["As a result, we can watch how the loss is decreasing while training. We can evaluate our model on the validation set. The perplexity is high because we only have to make predictions for the masked tokens (which represent 15% of the total here)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104},"id":"4lI4mgajKz70","executionInfo":{"status":"ok","timestamp":1632657448210,"user_tz":-120,"elapsed":5550,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"7804154e-6588-4577-c296-ad44ab613497"},"source":["eval_results = trainer.evaluate()\n","print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1441\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='181' max='181' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [181/181 00:05]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Perplexity: 11.10\n"]}]},{"cell_type":"markdown","metadata":{"id":"_ZkooHz1-_2h"},"source":["## Save our final model and tokenizer to disk\n","\n","Save the model and tokenizer ina way that they can be restored for a future downstream task, our encoder decoder model"]},{"cell_type":"code","metadata":{"id":"QDNgPls7_l13","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632657523966,"user_tz":-120,"elapsed":3129,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"2ec32780-cba2-4243-ee34-6163a32e3569"},"source":["trainer.save_model(model_folder)"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM\n","Configuration saved in /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/config.json\n","Model weights saved in /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/pytorch_model.bin\n"]}]},{"cell_type":"markdown","metadata":{"id":"d0caceCy_p1-"},"source":["# Checking the trained model using a Pipeline"]},{"cell_type":"markdown","metadata":{"id":"iIQJ8ND_AEhl"},"source":["Looking at the training and eval losses going down is not enough, we would like to apply our model to check if our language model is learning anything interesting. An easy way is via the FillMaskPipeline.\n","\n","Pipelines are simple wrappers around tokenizers and models. **We can use the 'fill-mask' pipeline** where we input a sequence containing a masked token (<mask>) and it returns a list of the most probable filled sequences, with their probabilities.\n"]},{"cell_type":"code","metadata":{"id":"ltXgXyCbAJLY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632657529212,"user_tz":-120,"elapsed":1217,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"46ce2555-cb14-4751-bf80-3ff2d43cc239"},"source":["from transformers import pipeline\n","\n","fill_mask = pipeline(\n","    \"fill-mask\",\n","    model=model_folder,\n","    tokenizer=tokenizer_folder\n",")"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.11.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 8192\n","}\n","\n","loading configuration file /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.11.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 8192\n","}\n","\n","loading weights file /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM/pytorch_model.bin\n","All model checkpoint weights were used when initializing RobertaForMaskedLM.\n","\n","All the weights of RobertaForMaskedLM were initialized from the model checkpoint at /content/drive/My Drive/Projects/text_generation_names/RoBERTaMLM.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n","Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file /content/drive/My Drive/Projects/text_generation_names/TokRoBERTa/config.json\n","/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n","  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.11.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 8192\n","}\n","\n","Didn't find file /content/drive/My Drive/Projects/text_generation_names/TokRoBERTa/tokenizer.json. We won't load it.\n","Didn't find file /content/drive/My Drive/Projects/text_generation_names/TokRoBERTa/added_tokens.json. We won't load it.\n","Didn't find file /content/drive/My Drive/Projects/text_generation_names/TokRoBERTa/special_tokens_map.json. We won't load it.\n","Didn't find file /content/drive/My Drive/Projects/text_generation_names/TokRoBERTa/tokenizer_config.json. We won't load it.\n","loading file /content/drive/My Drive/Projects/text_generation_names/TokRoBERTa/vocab.json\n","loading file /content/drive/My Drive/Projects/text_generation_names/TokRoBERTa/merges.txt\n","loading file None\n","loading file None\n","loading file None\n","loading file None\n","loading configuration file /content/drive/My Drive/Projects/text_generation_names/TokRoBERTa/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.11.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 8192\n","}\n","\n","loading configuration file /content/drive/My Drive/Projects/text_generation_names/TokRoBERTa/config.json\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.11.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 8192\n","}\n","\n"]}]},{"cell_type":"code","metadata":{"id":"UIvgZ3S6AO0z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632657532389,"user_tz":-120,"elapsed":228,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"d8d08b9c-9eee-43a8-f341-fefe4c4b25c7"},"source":["# knit midi dress with vneckline\n","# =>\n","fill_mask(\"midi <mask> with vneckline.\")"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.738255500793457,\n","  'sequence': 'midi dress with vneckline.',\n","  'token': 413,\n","  'token_str': ' dress'},\n"," {'score': 0.23444007337093353,\n","  'sequence': 'midi skirt with vneckline.',\n","  'token': 595,\n","  'token_str': ' skirt'},\n"," {'score': 0.002560280729085207,\n","  'sequence': 'midi sleeve with vneckline.',\n","  'token': 485,\n","  'token_str': ' sleeve'},\n"," {'score': 0.00225968100130558,\n","  'sequence': 'midi nightdress with vneckline.',\n","  'token': 3057,\n","  'token_str': ' nightdress'},\n"," {'score': 0.002166199963539839,\n","  'sequence': 'midi crop with vneckline.',\n","  'token': 1356,\n","  'token_str': ' crop'}]"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"i0qCyyhNAWZi"},"source":["Ok, simple syntax/grammar works. Let’s try a slightly more interesting prompt:\n","\n"]},{"cell_type":"code","metadata":{"id":"YZ9HSQxAAbme","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632657536183,"user_tz":-120,"elapsed":179,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"7082da83-0ed3-4672-f1b3-c77c5468a0f5"},"source":["# The test text: Round neck sweater with long sleeves\n","fill_mask(\"Round neck sweater with <mask> sleeves.\")"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.9134358167648315,\n","  'sequence': 'Round neck sweater with long sleeves.',\n","  'token': 355,\n","  'token_str': ' long'},\n"," {'score': 0.08268018811941147,\n","  'sequence': 'Round neck sweater with short sleeves.',\n","  'token': 418,\n","  'token_str': ' short'},\n"," {'score': 0.0003968868113588542,\n","  'sequence': 'Round neck sweater with turnup sleeves.',\n","  'token': 879,\n","  'token_str': ' turnup'},\n"," {'score': 0.0003081343602389097,\n","  'sequence': 'Round neck sweater with puff sleeves.',\n","  'token': 713,\n","  'token_str': ' puff'},\n"," {'score': 0.00026008510030806065,\n","  'sequence': 'Round neck sweater with ruffled sleeves.',\n","  'token': 741,\n","  'token_str': ' ruffled'}]"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"xHnzqLGk-PLK"},"source":[""],"execution_count":null,"outputs":[]}]}