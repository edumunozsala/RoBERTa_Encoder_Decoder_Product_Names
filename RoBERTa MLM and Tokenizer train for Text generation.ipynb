{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBERTa MLM and Tokenizer train for Text generation.ipynb","provenance":[{"file_id":"https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb","timestamp":1612196531906}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"M1oqh0F6W3ad"},"source":["# Train a language model (Masked Language Modelling) from scratch using Huggingface Transformers and a custom tokenizer\n","\n","### Extracted from the notebook (link to blogpost [link](https://huggingface.co/blog/how-to-train)).\n","\n","\n","Over the past few months, we made several improvements to our [`transformers`](https://github.com/huggingface/transformers) and [`tokenizers`](https://github.com/huggingface/tokenizers) libraries, with the goal of making it easier than ever to **train a new language model from scratch**.\n","\n","In this post we’ll demo how to train a “small” RoBERTa model (6 layers, 768 hidden size, 12 attention heads) – that’s the same number of layers & heads as DistilBERT – on a language from a clothing shop. We’ll then fine-tune the model on a downstream task of text generation.\n"]},{"cell_type":"markdown","metadata":{"id":"flij-vszUsQm"},"source":["# Loading the libraries"]},{"cell_type":"code","metadata":{"id":"mc6KQfWIUuZB"},"source":["import os\n","import pandas as pd\n","import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tvVMVDYjUaRt"},"source":["# Loading the datasets"]},{"cell_type":"code","metadata":{"id":"HOk4iZ9YZvec","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614415994847,"user_tz":-60,"elapsed":23627,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"d93a83bc-6950-4f2c-ddae-9028e15b0d58"},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5TwDqHbL9SY0"},"source":["Set the variables to the data folders:"]},{"cell_type":"code","metadata":{"id":"f2tjpFnyU-8T"},"source":["#Set the path to the data folder, datafile and output folder and files\n","root_folder = '/content/drive/My Drive/'\n","data_folder = os.path.abspath(os.path.join(root_folder, 'datasets/text_gen_product_names'))\n","model_folder = os.path.abspath(os.path.join(root_folder, 'Projects/text_generation_names/RoBERTaML'))\n","output_folder = os.path.abspath(os.path.join(root_folder, 'Projects/text_generation_names'))\n","tokenizer_folder = os.path.abspath(os.path.join(root_folder, 'Projects/text_generation_names/TokRoBERTa'))\n","\n","test_filename='cl_test_descriptions.csv'\n","datafile= 'product_names_desc_cl_train.csv'\n","outputfile = 'submission.csv'\n","\n","datafile_path = os.path.abspath(os.path.join(data_folder,datafile))\n","testfile_path = os.path.abspath(os.path.join(data_folder,test_filename))\n","outputfile_path = os.path.abspath(os.path.join(output_folder,outputfile))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nx_diDArVIHa"},"source":["Load the train datafile with the product descriptions and names:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MqxDq-LcVL0T","executionInfo":{"status":"ok","timestamp":1614416165407,"user_tz":-60,"elapsed":1552,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"15acf6c2-f016-4565-ca5e-5005d29e4943"},"source":["# Load the dataset: sentence in english, sentence in spanish \n","train_df=pd.read_csv(datafile_path, header=0, usecols=[0,1])\n","print('Num Examples: ',len(train_df))\n","print('Null Values\\n', train_df.isna().sum())\n","train_df.dropna(inplace=True)\n","print('Num Examples: ',len(train_df))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Num Examples:  31593\n","Null Values\n"," name           44\n","description     1\n","dtype: int64\n","Num Examples:  31548\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AGjcMQ_J9yXu"},"source":["Then, we read the test dataset:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pbCZSLQxVYr-","executionInfo":{"status":"ok","timestamp":1614416169224,"user_tz":-60,"elapsed":1018,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"90065caf-4d03-4a6b-c06d-0a8e1fcd1d16"},"source":["# Load the dataset: sentence in english, sentence in spanish \n","test_df=pd.read_csv(testfile_path, header=0)\n","print('Num Examples: ',len(test_df))\n","print('Null Values\\n', test_df.isna().sum())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Num Examples:  1441\n","Null Values\n"," description    0\n","dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mv3X5LvKVtAN"},"source":["To train our Tokenizer we need to save every text example in our dataset to a txt file, including both the train and test dataset:"]},{"cell_type":"code","metadata":{"id":"FU9C8eNEc1jz"},"source":["txt_files_dir = \"./text_split\"\n","!rm -rf {txt_files_dir}\n","!mkdir {txt_files_dir}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GW5loLTdtUy"},"source":["# Store values in a dataframe column (Series object) to files, one file per record\n","def column_to_files(column, prefix, txt_files_dir):\n","    i=prefix\n","    #For every value in the df, with just one column\n","    for row in column.to_list():\n","      file_name = os.path.join(txt_files_dir, str(i)+'.txt')\n","      try:\n","        f = open(file_name, 'wb')\n","        f.write(row.encode('utf-8'))\n","        f.close()\n","      except Exception as e:  #catch exceptions(for eg. empty rows)\n","        print(row, e) \n","      i+=1\n","    \n","    return i\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AurkxAD-Vx-M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614416189462,"user_tz":-60,"elapsed":1557,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"3e5ea5b7-1f8d-484d-9e2e-31d4cf86dac9"},"source":["data = train_df[\"description\"]\n","data = data.replace(\"\\n\",\" \")\n","prefix=0\n","prefix = column_to_files(data, prefix, txt_files_dir)\n","print(prefix)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["31548\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ldqNgudjd540","executionInfo":{"status":"ok","timestamp":1614416199868,"user_tz":-60,"elapsed":477,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"54df3bde-3a02-4df7-e942-ccbf529d91d6"},"source":["data = test_df[\"description\"]\n","data = data.replace(\"\\n\",\" \")\n","print(len(data))\n","prefix = column_to_files(data, prefix, txt_files_dir)\n","print(prefix)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1441\n","32989\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7rxjGQJQox_y","executionInfo":{"status":"ok","timestamp":1614416233792,"user_tz":-60,"elapsed":1360,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"6b61da99-f021-4879-ab19-e7b77eea1446"},"source":["data = train_df[\"name\"]\n","data = data.replace(\"\\n\",\" \")\n","print(len(data))\n","prefix = column_to_files(data, prefix, txt_files_dir)\n","print(prefix)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["31548\n","64537\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"G-kkz81OY6xH"},"source":["# 3. Train a tokenizer\n","\n","We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let’s pick its size to be 8,192 because our specific vocabulary is very limited and simple.\n","\n","We recommend training a byte-level BPE (rather than let’s say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `<unk>` tokens!).\n"]},{"cell_type":"code","metadata":{"id":"5duRggBRZKvP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614416315831,"user_tz":-60,"elapsed":52574,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"e19d691e-1821-4d87-9168-a6a55e9eb97f"},"source":["# We won't need TensorFlow here\n","!pip uninstall -y tensorflow\n","# Install `transformers` from master\n","!pip install git+https://github.com/huggingface/transformers\n","!pip list | grep -E 'transformers|tokenizers'\n","# transformers version at notebook update --- 2.11.0\n","# tokenizers version at notebook update --- 0.8.0rc1\n","!pip install datasets==1.0.2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Uninstalling tensorflow-2.4.1:\n","  Successfully uninstalled tensorflow-2.4.1\n","Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-sveetl9v\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-sveetl9v\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (2019.12.20)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (3.7.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (20.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (4.41.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 8.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (1.19.5)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 50.9MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.0.dev0) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0.dev0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.0.1)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.4.0.dev0-cp37-none-any.whl size=1891037 sha256=f30753be778bc55cce8a926f98740744e118433ddf48ea2e811f0c42401ba857\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-frqg7duv/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n","Successfully built transformers\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=701a16cb098f533f408d7499b1336edef46d27a735d835b4542422dc6e458139\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.0.dev0\n","tokenizers                    0.10.1        \n","transformers                  4.4.0.dev0    \n","Collecting datasets==1.0.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/7e/8d9e2fd30e3819e6042927d379f3668a0b49fe38b92d5639194808a1d877/datasets-1.0.2-py3-none-any.whl (1.8MB)\n","\u001b[K     |████████████████████████████████| 1.8MB 8.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (4.41.1)\n","Collecting xxhash\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/27/1c0b37c53a7852f1c190ba5039404d27b3ae96a55f48203a74259f8213c9/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n","\u001b[K     |████████████████████████████████| 245kB 50.2MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (1.1.5)\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (3.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (3.0.12)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.2) (0.3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.0.2) (1.24.3)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.0.2) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.0.2) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.15.0)\n","Installing collected packages: xxhash, datasets\n","Successfully installed datasets-1.0.2 xxhash-2.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QgdLhEQS4c5f"},"source":["from pathlib import Path\n","\n","from tokenizers import ByteLevelBPETokenizer\n","from tokenizers.processors import BertProcessing\n","\n","from datasets import Dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CzKv-3nH66yq"},"source":["Now we can train our tokenizer on the text files containing our vocabulary"]},{"cell_type":"code","metadata":{"id":"IMnymRDLe0hi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614416353562,"user_tz":-60,"elapsed":2644,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"f4b97329-7b0a-4d22-d768-5f5d4110317e"},"source":["%%time \n","paths = [str(x) for x in Path(\".\").glob(\"text_split/*.txt\")]\n","\n","# Initialize a tokenizer\n","tokenizer = ByteLevelBPETokenizer(lowercase=True)\n","\n","# Customize training\n","tokenizer.train(files=paths, vocab_size=8192, min_frequency=1,\n","                show_progress=True,\n","                special_tokens=[\n","                                \"<s>\",\n","                                \"<pad>\",\n","                                \"</s>\",\n","                                \"<unk>\",\n","                                \"<mask>\",\n","])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 3.77 s, sys: 3.32 s, total: 7.1 s\n","Wall time: 2.19 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JBCOT9aI98fl","executionInfo":{"status":"ok","timestamp":1614416356429,"user_tz":-60,"elapsed":512,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"cd35c3f0-4c3b-4350-b759-85b492201db0"},"source":["tokenizer"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Tokenizer(vocabulary_size=8192, model=ByteLevelBPE, add_prefix_space=False, lowercase=True, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"6Ei7bqpRf1LH"},"source":["Now let's save the tokenizer to disk, later we will use it to train the language model:"]},{"cell_type":"code","metadata":{"id":"EIS-irI0f32P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614416388796,"user_tz":-60,"elapsed":1431,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"d9798f66-5914-419b-e32a-9258cf16da0c"},"source":["#Save the Tokenizer to disk\n","tokenizer.save_model(tokenizer_folder)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/My Drive/Projects/SpainAI NLP/TokRoBERTa/vocab.json',\n"," '/content/drive/My Drive/Projects/SpainAI NLP/TokRoBERTa/merges.txt']"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"lOOfYSuQhSqT"},"source":["The count of samples is small and the tokenizer trains very fast.\n","\n","We now have both a `vocab.json`, which is a list of the most frequent tokens ranked by frequency, and a `merges.txt` list of merges.\n","\n","```json\n","{\n","\t\"<s>\": 0,\n","\t\"<pad>\": 1,\n","\t\"</s>\": 2,\n","\t\"<unk>\": 3,\n","\t\"<mask>\": 4,\n","\t\"!\": 5,\n","\t\"\\\"\": 6,\n","\t\"#\": 7,\n","\t\"$\": 8,\n","\t\"%\": 9,\n","\t\"&\": 10,\n","\t\"'\": 11,\n","\t\"(\": 12,\n","\t\")\": 13,\n","\t# ...\n","}\n","\n","# merges.txt\n","l a\n","Ġ k\n","o n\n","Ġ la\n","t a\n","Ġ e\n","Ġ d\n","Ġ p\n","# ...\n","```\n","\n","What is great is that our tokenizer is optimized for our very specific vocabulary. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. \n","\n","Here’s  how you can use it in `tokenizers`, including handling the RoBERTa special tokens – of course, you’ll also be able to use it directly from `transformers`.\n"]},{"cell_type":"code","metadata":{"id":"tKVWB8WShT-z"},"source":["tokenizer = ByteLevelBPETokenizer(\n","    os.path.abspath(os.path.join(tokenizer_folder,'vocab.json')),\n","    os.path.abspath(os.path.join(tokenizer_folder,'merges.txt'))\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hO5M3vrAhcuj"},"source":["tokenizer._tokenizer.post_processor = BertProcessing(\n","    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",")\n","tokenizer.enable_truncation(max_length=512)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Z2IxmcK_eSf"},"source":["Let's show some examples:"]},{"cell_type":"code","metadata":{"id":"E3Ye27nchfzq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613038796961,"user_tz":-60,"elapsed":854,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"2ff0e4c2-8fca-492e-a04c-4e388345aa8d"},"source":["tokenizer.encode(\"knit midi dress with vneckline straps.\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"X8ya5_7rhjKS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613038800283,"user_tz":-60,"elapsed":995,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"606276c3-b6b5-465a-80f3-9842543bb06f"},"source":["tokenizer.encode(\"knit midi dress with vneckline straps.\").tokens"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<s>',\n"," 'knit',\n"," 'Ġmidi',\n"," 'Ġdress',\n"," 'Ġwith',\n"," 'Ġvneckline',\n"," 'Ġstraps',\n"," '.',\n"," '</s>']"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"WQpUC_CDhnWW"},"source":["# 4. Train a language model from scratch\n","\n","**Update:** This section follows along the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, using our new [`Trainer`](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) directly. Feel free to pick the approach you like best.\n","\n","> We’ll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the [documentation](https://huggingface.co/transformers/model_doc/roberta.html) for more details).\n","\n","As the model is BERT-like, we’ll train it on a task of *Masked language modeling*, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script.\n"]},{"cell_type":"code","metadata":{"id":"V6VsZnOd636F"},"source":["TRAIN_BATCH_SIZE = 16    # input batch size for training (default: 64)\n","VALID_BATCH_SIZE = 8    # input batch size for testing (default: 1000)\n","TRAIN_EPOCHS = 25        # number of epochs to train (default: 10)\n","LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n","SEED = 42               # random seed (default: 42)\n","MAX_LEN = 100\n","SUMMARY_LEN = 7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kD140sFjh0LQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614416407570,"user_tz":-60,"elapsed":501,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"dcc93ecb-54d7-4d84-81a7-612992dc6773"},"source":["# Check that we have a GPU\n","!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sat Feb 27 09:00:07 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VNZZs-r6iKAV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614416410606,"user_tz":-60,"elapsed":496,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"7d9f4676-d19b-4193-c87a-f31a9888b7c6"},"source":["# Check that PyTorch sees it\n","import torch\n","torch.cuda.is_available()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"u0qQzgrBi1OX"},"source":["### We'll define the following config for the model"]},{"cell_type":"code","metadata":{"id":"LTXXutqeDzPi"},"source":["from transformers import RobertaConfig\n","\n","config = RobertaConfig(\n","    vocab_size=8192,\n","    max_position_embeddings=514,\n","    num_attention_heads=12,\n","    num_hidden_layers=6,\n","    type_vocab_size=1,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yAwQ82JiE5pi"},"source":["Now let's re-create our tokenizer in transformers"]},{"cell_type":"code","metadata":{"id":"4keFBUjQFOD1"},"source":["from transformers import RobertaTokenizerFast\n","\n","tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_folder, max_len=128)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RgXwwfgbG8WN","executionInfo":{"status":"ok","timestamp":1614416438140,"user_tz":-60,"elapsed":472,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"02e84c9b-2b51-41c9-c46e-e9113282f870"},"source":["tokenizer"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PreTrainedTokenizerFast(name_or_path='/content/drive/My Drive/Projects/SpainAI NLP/TokRoBERTa', vocab_size=8192, model_max_len=128, is_fast=True, padding_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"6yNCw-3hFv9h"},"source":["Finally let's initialize our model.\n","\n","**Important:**\n","\n","As we are training from scratch, we only initialize from a config, not from an existing pretrained model or checkpoint."]},{"cell_type":"code","metadata":{"id":"BzMqR-dzF4Ro"},"source":["from transformers import RobertaForMaskedLM\n","\n","model = RobertaForMaskedLM(config=config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jU6JhBSTKiaM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614420816691,"user_tz":-60,"elapsed":511,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"6ee64b19-b9fc-4dda-c947-87a9060269cc"},"source":["model.num_parameters()\n","# => 49 million parameters"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["49816064"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"jBtUHRMliOLM"},"source":["### Now let's build our training Dataset\n","\n","We'll build our dataset by applying our tokenizer to our text file.\n","\n","Here, as we only have one text file, we don't even need to customize our `Dataset`. We'll just use the `LineByLineDataset` out-of-the-box."]},{"cell_type":"code","metadata":{"id":"6YLQrEK8EoHC"},"source":["train_files_dir = \"./train_files\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PPcwSxp0AGHH"},"source":["Recreate the folder to contain the text files:"]},{"cell_type":"code","metadata":{"id":"AyR5ymHT_u3N"},"source":["!rm -rf {train_files_dir}\n","!mkdir {train_files_dir}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z4wIfHwwAXNW"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0lSPCYCc_hbG","executionInfo":{"status":"ok","timestamp":1614420941641,"user_tz":-60,"elapsed":491,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"964d8d87-e278-4833-ae8b-aed8ae2c8adc"},"source":["train_split = 1\n","train_data_size = int(len(train_df)*train_split)\n","test_data_size = int(len(test_df)*train_split)\n","print('Len Train data: ', str(train_data_size),' Len Test data: ', str(test_data_size))\n","\n","with open(os.path.join(train_files_dir,'train.txt') , 'w') as f:\n","    for item in train_df[:train_data_size].name.to_list():\n","        f.write(\"%s\\n\" % item)\n","    # We can evaluate to use the test file to train our language model\n","    #for item in test_df[:test_data_size].description.to_list():\n","    #    f.write(\"%s\\n\" % item)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Len Train data:  31548  Len Test data:  1441\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BJKYPraPDl0r"},"source":["### Create our dataset from the text file `train.txt`"]},{"cell_type":"code","metadata":{"id":"GlvP_A-THEEl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614420983776,"user_tz":-60,"elapsed":1120,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"df33266c-d6f6-44c2-8363-53e608577401"},"source":["%%time\n","from transformers import LineByLineTextDataset\n","\n","dataset = LineByLineTextDataset(\n","    tokenizer=tokenizer,\n","    file_path=os.path.join(train_files_dir,'train.txt'),\n","    block_size=32,\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:128: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["CPU times: user 1.03 s, sys: 30.7 ms, total: 1.06 s\n","Wall time: 618 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KcJmT-v_BKvR"},"source":["## TO DO: **CREATE A DATASET TO TRAIN INSTEAD OF FROM FILES**"]},{"cell_type":"markdown","metadata":{"id":"XBdFLR-dZnXs"},"source":["https://ryanong.co.uk/2020/06/11/day-163-how-to-build-a-language-model-from-scratch-implementation/"]},{"cell_type":"code","metadata":{"id":"WFMmfy_LDpRZ"},"source":["class EsperantoDataset(Dataset):\n","    def __init__(self, evaluate: bool = False):\n","        tokenizer = ByteLevelBPETokenizer(\n","            \"./esperberto/vocab.json\",\n","            \"./esperberto/merges.txt\",\n","        )\n","        tokenizer._tokenizer.post_processor = BertProcessing(\n","            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n","            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n","        )\n","        tokenizer.enable_truncation(max_length=512)\n","        # or use the RobertaTokenizer from `transformers` directly.\n","\n","        self.examples = []\n","\n","        src_files = Path(\"./data/\").glob(\"*-eval.txt\") if evaluate else Path(\"./data/\").glob(\"final_data.txt\")\n","        for src_file in src_files:\n","            print(\"🔥\", src_file)\n","            lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n","            self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, i):\n","        # We’ll pad at the batch level.\n","        return torch.tensor(self.examples[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0miQ7KOQ3eHp","executionInfo":{"status":"ok","timestamp":1612691426902,"user_tz":-60,"elapsed":654,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"7c84f1b2-b7fd-477a-89b4-f488d7d8043a"},"source":["# Concatenate the train dataset and the test dataset for language modelling\n","df=pd.concat([train_df['description'], test_df['description']], axis=0)\n","print(len(df), len(train_df), len(test_df))\n","print(df.head(5))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["32989 31548 1441\n","0       towel with border with lines metallic thread .\n","1    printed bermuda shorts made technical fabric ....\n","2    bodysuit with shapewear effect . this bodysuit...\n","3    vneck with thin adjustable straps.height model...\n","4    puritan collar dress featuring long sleeves wi...\n","Name: description, dtype: object\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5hsYEUiZ5w5O","executionInfo":{"status":"ok","timestamp":1612692430937,"user_tz":-60,"elapsed":430,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"66f8dfbb-a894-4c3c-b1bf-4d75e01f88c0"},"source":["df[:100]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0        towel with border with lines metallic thread .\n","1     printed bermuda shorts made technical fabric ....\n","2     bodysuit with shapewear effect . this bodysuit...\n","3     vneck with thin adjustable straps.height model...\n","4     puritan collar dress featuring long sleeves wi...\n","                            ...                        \n","95                            bracelet with appliqués .\n","96    teardropshaped dangle earrings with rhinestone...\n","97    round neck tshirt with short sleeves . contras...\n","98                 pack pairs plain socks.one size only\n","99    flat mules caramel . tortoiseshelleffect vinyl...\n","Name: description, Length: 100, dtype: object"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CzGP_bx_3PyK","executionInfo":{"status":"ok","timestamp":1612692641476,"user_tz":-60,"elapsed":648,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"1cda6831-dc53-43a2-86bd-458c277cecf3"},"source":["dataset=Dataset.from_pandas(df.to_frame())\n","dataset.remove_columns_(['__index_level_0__'])\n","print(dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dataset(features: {'description': Value(dtype='string', id=None)}, num_rows: 32989)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bApQs09y9oY6","executionInfo":{"status":"ok","timestamp":1612692545216,"user_tz":-60,"elapsed":603,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"d54e909e-96fb-4df2-f206-75d1518d8a01"},"source":["dataset.set_format(type='torch', columns=['description'])\n","dataset.remove_columns_(['__index_level_0__'])\n","print(dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Dataset(features: {'description': Value(dtype='string', id=None), '__index_level_0__': Value(dtype='int64', id=None)}, num_rows: 32989)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ACOfK4avA_qx"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5aTvI2sBDK5N","executionInfo":{"status":"ok","timestamp":1614420988797,"user_tz":-60,"elapsed":497,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"e10226e4-2aaa-4af2-81d7-e521b56c2b87"},"source":["dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<transformers.data.datasets.language_modeling.LineByLineTextDataset at 0x7fe8d19de4d0>"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"Cm5cA1XUBkNB"},"source":["## Define the Data Collactor for masking our language"]},{"cell_type":"markdown","metadata":{"id":"hDLs73HcIHk5"},"source":["Like in the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, we need to define a data_collator.\n","\n","This is just a small helper that will help us batch different samples of the dataset together into an object that PyTorch knows how to perform backprop on."]},{"cell_type":"code","metadata":{"id":"zTgWPa9Dipk2"},"source":["from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ri2BIQKqjfHm"},"source":["### Finally, we are all set to initialize our Trainer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2EDjWLc1HdQp","executionInfo":{"status":"ok","timestamp":1614421055117,"user_tz":-60,"elapsed":482,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"b8f81f14-2c73-447a-836e-89ed50e29bc4"},"source":["model_folder = os.path.abspath(os.path.join(root_folder, 'Projects/SpainAI NLP/DecRoBERTaML'))\n","print(model_folder)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Projects/SpainAI NLP/DecRoBERTaML\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zZhOfZ-RByBr"},"source":["Set the training arguments for our model:"]},{"cell_type":"code","metadata":{"id":"YpvnFFmZJD-N"},"source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir=model_folder,\n","    overwrite_output_dir=True,\n","    num_train_epochs=15,\n","    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n","    save_steps=8192,\n","    eval_steps=4096,\n","    save_total_limit=1,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset,\n","    #prediction_loss_only=True,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o6sASa36Nf-N"},"source":["### Start training"]},{"cell_type":"code","metadata":{"id":"VmaHZXzmkNtJ","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1614422283288,"user_tz":-60,"elapsed":1177918,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"afad3587-b722-4b14-9aa3-8db8b6ab5453"},"source":["trainer.train()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","        </style>\n","      \n","      <progress value='29580' max='29580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [29580/29580 19:37, Epoch 15/15]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>7.250100</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>6.373800</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>6.078600</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>5.872800</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>5.621200</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>5.494000</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>5.342800</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>5.258100</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>5.031000</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>4.963200</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>4.830200</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>4.749100</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>4.646400</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>4.502800</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>4.439400</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>4.337200</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>4.310500</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>4.304400</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>4.232900</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>4.094900</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>4.112400</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>4.002800</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>4.036900</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>3.933100</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>3.922600</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>3.864400</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>3.918900</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>3.721200</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>3.717500</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>3.719200</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>3.747100</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>3.686900</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>3.563900</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>3.556400</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>3.598100</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>3.547600</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>3.493700</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>3.498600</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>3.446300</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>3.441700</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>3.335100</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>3.346600</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>3.378000</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>3.373300</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>3.326800</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>3.294500</td>\n","    </tr>\n","    <tr>\n","      <td>23500</td>\n","      <td>3.261200</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>3.236600</td>\n","    </tr>\n","    <tr>\n","      <td>24500</td>\n","      <td>3.267200</td>\n","    </tr>\n","    <tr>\n","      <td>25000</td>\n","      <td>3.182700</td>\n","    </tr>\n","    <tr>\n","      <td>25500</td>\n","      <td>3.195600</td>\n","    </tr>\n","    <tr>\n","      <td>26000</td>\n","      <td>3.192700</td>\n","    </tr>\n","    <tr>\n","      <td>26500</td>\n","      <td>3.201300</td>\n","    </tr>\n","    <tr>\n","      <td>27000</td>\n","      <td>3.244100</td>\n","    </tr>\n","    <tr>\n","      <td>27500</td>\n","      <td>3.224400</td>\n","    </tr>\n","    <tr>\n","      <td>28000</td>\n","      <td>3.191500</td>\n","    </tr>\n","    <tr>\n","      <td>28500</td>\n","      <td>3.131700</td>\n","    </tr>\n","    <tr>\n","      <td>29000</td>\n","      <td>3.082100</td>\n","    </tr>\n","    <tr>\n","      <td>29500</td>\n","      <td>3.121200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=29580, training_loss=4.028654971067933, metrics={'train_runtime': 1177.1714, 'train_samples_per_second': 25.128, 'total_flos': 1187514138046464.0, 'epoch': 15.0, 'init_mem_cpu_alloc_delta': 47230, 'init_mem_gpu_alloc_delta': 0, 'init_mem_cpu_peaked_delta': 18306, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 326984, 'train_mem_gpu_alloc_delta': 599612928, 'train_mem_cpu_peaked_delta': 52578005, 'train_mem_gpu_peaked_delta': 200973312})"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"_ZkooHz1-_2h"},"source":["#### 🎉 Save final model (+ tokenizer + config) to disk"]},{"cell_type":"code","metadata":{"id":"QDNgPls7_l13"},"source":["trainer.save_model(model_folder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d0caceCy_p1-"},"source":["## 4. Check that the LM actually trained"]},{"cell_type":"markdown","metadata":{"id":"iIQJ8ND_AEhl"},"source":["Aside from looking at the training and eval losses going down, the easiest way to check whether our language model is learning anything interesting is via the `FillMaskPipeline`.\n","\n","Pipelines are simple wrappers around tokenizers and models, and the 'fill-mask' one will let you input a sequence containing a masked token (here, `<mask>`) and return a list of the most probable filled sequences, with their probabilities.\n","\n"]},{"cell_type":"code","metadata":{"id":"ltXgXyCbAJLY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613067164735,"user_tz":-60,"elapsed":3944,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"b835bb10-e797-4fe6-eb1f-ad8e1c8dccf0"},"source":["from transformers import pipeline\n","\n","fill_mask = pipeline(\n","    \"fill-mask\",\n","    model=os.path.abspath(os.path.join(output_folder,'TokRoBERTa')),\n","    tokenizer=os.path.abspath(os.path.join(output_folder,'TokRoBERTa'))\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/My Drive/Projects/SpainAI NLP/TokRoBERTa and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"UIvgZ3S6AO0z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613067167134,"user_tz":-60,"elapsed":723,"user":{"displayName":"Eduardo Muñoz Sala","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi9gTFB7xfdxQyNiKRUzqE5vwb9WQjQuMtQz5F16w=s64","userId":"13317831924226771761"}},"outputId":"7075b8eb-66cc-4cb3-99db-4769e43ea9a7"},"source":["# knit midi dress with vneckline\n","# =>\n","fill_mask(\"midi <mask> with vneckline.\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.37433964014053345,\n","  'sequence': 'midi dress with vneckline.',\n","  'token': 482,\n","  'token_str': ' dress'},\n"," {'score': 0.33222395181655884,\n","  'sequence': 'midi skirt with vneckline.',\n","  'token': 769,\n","  'token_str': ' skirt'},\n"," {'score': 0.035536717623472214,\n","  'sequence': 'midi crop with vneckline.',\n","  'token': 1693,\n","  'token_str': ' crop'},\n"," {'score': 0.023702150210738182,\n","  'sequence': 'midi sleeve with vneckline.',\n","  'token': 469,\n","  'token_str': ' sleeve'},\n"," {'score': 0.0199593435972929,\n","  'sequence': 'midi vest with vneckline.',\n","  'token': 2315,\n","  'token_str': ' vest'}]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"i0qCyyhNAWZi"},"source":["Ok, simple syntax/grammar works. Let’s try a slightly more interesting prompt:\n","\n"]},{"cell_type":"code","metadata":{"id":"YZ9HSQxAAbme","colab":{"base_uri":"https://localhost:8080/","height":283},"outputId":"aabfeedc-b1d0-4837-b01d-cd42726a5a3d"},"source":["fill_mask(\"Jen la komenco de bela <mask>.\")\n","\n","# This is the beginning of a beautiful <mask>.\n","# =>"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.01814725436270237,\n","  'sequence': '<s> Jen la komenco de bela urbo.</s>',\n","  'token': 871},\n"," {'score': 0.015888698399066925,\n","  'sequence': '<s> Jen la komenco de bela vivo.</s>',\n","  'token': 1160},\n"," {'score': 0.015662025660276413,\n","  'sequence': '<s> Jen la komenco de bela tempo.</s>',\n","  'token': 1021},\n"," {'score': 0.015555007383227348,\n","  'sequence': '<s> Jen la komenco de bela mondo.</s>',\n","  'token': 945},\n"," {'score': 0.01412549614906311,\n","  'sequence': '<s> Jen la komenco de bela tago.</s>',\n","  'token': 1633}]"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"6RsGaD1qAfLP"},"source":["## 5. Share your model 🎉"]},{"cell_type":"markdown","metadata":{"id":"wZsyV4v990Ss"},"source":["Finally, when you have a nice model, please think about sharing it with the community:\n","\n","- upload your model using the CLI: `transformers-cli upload`\n","- write a README.md model card and add it to the repository under `model_cards/`. Your model card should ideally include:\n","    - a model description,\n","    - training params (dataset, preprocessing, hyperparameters), \n","    - evaluation results,\n","    - intended uses & limitations\n","    - whatever else is helpful! 🤓"]}]}